In my project, the HRTF sound is implemented as a sound object attached to an object extending the VizNode class. My objectives were to get practice writing CUDA and C code, but this project has also given me more experience with the inner workings of Vizard, audio engineering, signal processing and how to use C and Python together.

The design is intentionally similar to how directional sound is done in standard Vizard scripts. The programmer creating a Vizard script must first create an object as a sound source, give the object a sound file and then play it. The function takes the wav file, the position of the listener (the Vizard MainView) and the sound object, and produces an HRTF-filtered sound.
1. Calculate the elevation and azimuth of the object relative to the MainView
2. Find the HRTF file for the elevation and azimuth. The HRTF files are taken from the KEMAR dummy measurements from MIT.
3. Read in the source sound.
4. Initialize the convolution kernel. The convolution kernel is written in PyCUDA. Kernels written in PyCUDA have a C-like syntax and are thus easy to use for those with experience programming in CUDA C.
5. Put the information back into CPU memory and build the wave file.
6. Play the sound in Vizard.

For the purpose of demonstration and testing, I used a smaller sound file and one sound source. The HRTF works well and finding the location based on the audio is easy. There are currently 3 versions of the sound object. The first one, nsndObj.py uses an iterative, CPU-only convolution. This is done as a basis for comparison. It is slow and clunky, though the code is simple to read and should demonstrate how convolution is done mathematically. The next is gpusndObj.py, which uses PyCUDA to accomplish naive convolution. The speedup in comparison to CPU-only iterative convolution is much quicker, though the overhead of running the CUDA driver and memory copying is larger. The fastest option appears to be using NumPy, a Python package using a C backend. The breakdown and possible solutions for this approach of using PyCUDA GPU acceleration are elaborated on below.

Using CUDA with PyCUDA has a very large overhead since the nvcc driver (how PyCUDA interfaces with the GPU) must be invoked every time the kernel is called. This is especially apparent using smaller audio files, though larger sound files should experience speedups of greater magnitude. There are a few ways to solve this:
1. Ideally, we could modify an existing sound engine for GPU spatial sound instead of calling custom functions within Vizard itself. Vizard's API isn't exposed when it comes to synchronizing high level events with sound, so this would avoid the problem of having to override some of Vizard's existing classes and functions in order to get high fidelity spatial sounds. Surveying the internet suggests there are many projects that are attempting to make GPU accelerated sound engines, but none of them seem to be sufficiently mature for usage yet. Nvidia VR Works is developing GPU sound currently.
2. An alternative approach is to simply write a full C or Fortran library and invoke CUDA inside these libraries instead of just another PyCUDA module. This might cut down on some of the time, but I personally think it might be a lot of trouble for little payoff compared to using current, efficient Python modules written with C backends such as NumPy or SciPy. I wrote a seperate module (soundObj.py) with NumPy and it proved to be much faster than PyCUDA for small files.
3. Utilizing just-in-time compilers might also work. Fortunately, there are options to use modules such as Numba which provide NumPy support as well as parallel CPU and GPU. Numba was considered an option, but there were compatibility issues with Vizard, and the techniques there did not lend themselves to finer level memory management, which was something I wanted to explore for this project.
4. In addition to the above, for large enough files, using the Fourier transform to perform the filtering will be faster than a convolution in the time domain.

A few more issues related to the launching of the nvcc driver are related to the design and workflow of Vizard. The design of Vizard's object and sound playing classes is not ideal if we want to launch the kernel as little as possible. Here are some examples:
1. Because the CUDA portion of the code needs to be launched and compiled in the nvcc driver, it is difficult to perform real-time HRTF. Objects in VIzard can move, necessitating frequent updates to the position calculations which need to be not only simply copied to device memory, but must also withstand the repeated overhead of launching the driver again and again for every change in position of both listener and source. This would perhaps be possible if there were some better way of synchronizing object model loading and audio streaming. Unfortunatly, it does not appear to be a feature of Vizard.
2.  Performing HRTF for multiple sound sources, while ordinarily a naturally parallelizable task, would be difficult with Vizard's current workflow design. Launching the driver and performing the convolution for all sound sources is not simple in Vizard, as each sound source has to be explicitly attached to an object, and sound operations appears to be handled on a per-object basis, such that it would likely necessitate re-writing the VizSonic module entirely, or perhaps exposing more of the sound module API.

Other than needing to initialize the nvcc compiler, some room for improvement still exists. For one, audio is distorted and there is some amplitude clipping. I have to experiment more with normalization techniques.